{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwSFbYtVI8OA"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "# from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"G:/Gallblader Diseases Dataset/train\"\n",
        "test_dir = \"G:/Gallblader Diseases Dataset/test\"\n",
        "val_dir = \"G:/Gallblader Diseases Dataset/val\""
      ],
      "metadata": {
        "id": "fI4DEcUbJUEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "input_shape=(64,64)\n",
        "\n",
        "datagen_train = ImageDataGenerator(rescale=1./255)\n",
        "datagen_test = ImageDataGenerator(rescale=1./255)\n",
        "datagen_val=ImageDataGenerator(rescale=1./255)\n",
        "batch_size = 64\n",
        "\n",
        "# datagen_train = ImageDataGenerator(rescale=1./255, validation_split=0.20)\n",
        "\n",
        "# generator_train = datagen_train.flow_from_directory(directory=train_dir,\n",
        "#                                                     target_size=input_shape,\n",
        "#                                                     batch_size=batch_size,\n",
        "#                                                     subset=\"training\",\n",
        "#                                                     shuffle=True)\n",
        "# generator_val = datagen_train.flow_from_directory(directory=train_dir,\n",
        "#                                                   target_size=input_shape,\n",
        "#                                                   batch_size=batch_size,\n",
        "#                                                   subset=\"validation\",\n",
        "#                                                   shuffle=True)\n",
        "\n",
        "\n",
        "generator_train = datagen_train.flow_from_directory(directory=train_dir,\n",
        "                                                    target_size=input_shape,\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    shuffle=True)\n",
        "\n",
        "generator_val = datagen_val.flow_from_directory(directory=test_dir,\n",
        "                                                  target_size=input_shape,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False)\n",
        "\n",
        "generator_test = datagen_test.flow_from_directory(directory=test_dir,\n",
        "                                                  target_size=input_shape,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  shuffle=False)"
      ],
      "metadata": {
        "id": "W_vmY3oKJcWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (64, 64, 3)\n",
        "num_classes=9"
      ],
      "metadata": {
        "id": "vrINl8VxJjCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0001\n",
        "weight_decay = 0.00001\n",
        "batch_size = 128\n",
        "num_epochs = 50\n",
        "image_size = 64  # We'll resize input images to this size\n",
        "patch_size = 3  # Size of the patches to be extract from the input images\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "projection_dim = 32\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim * 2,\n",
        "    projection_dim,\n",
        "]  # Size of the transformer layers\n",
        "transformer_layers = 8\n",
        "mlp_head_units = [1024, 512]  # Size of the dense layers of the final classifier"
      ],
      "metadata": {
        "id": "TuCztjwfJrCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "26zQga9QJvhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Patches(layers.Layer):\n",
        "    def __init__(self, patch_size):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "    def call(self, images):\n",
        "        batch_size = tf.shape(images)[0]\n",
        "        patches = tf.image.extract_patches(\n",
        "            images=images,\n",
        "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
        "            strides=[1, self.patch_size, self.patch_size, 1],\n",
        "            rates=[1, 1, 1, 1],\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        patch_dims = patches.shape[-1]\n",
        "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "        return patches"
      ],
      "metadata": {
        "id": "4rJS5ZuiJ1-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "    def __init__(self, num_patches, projection_dim):\n",
        "        super(PatchEncoder, self).__init__()\n",
        "        self.num_patches = num_patches\n",
        "        self.projection = layers.Dense(units=projection_dim)\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_patches, output_dim=projection_dim\n",
        "        )\n",
        "\n",
        "    def call(self, patch):\n",
        "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
        "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
        "        return encoded"
      ],
      "metadata": {
        "id": "fQA7t1miJ7iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit_classifier():\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # # Augment data.\n",
        "    # augmented = data_augmentation(inputs)\n",
        "    # Create patches.\n",
        "    patches = Patches(patch_size)(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
        "        # Skip connection 2.\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Create a [batch_size, projection_dim] tensor.\n",
        "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "    representation = layers.Flatten()(representation)\n",
        "    representation = layers.Dropout(0.5)(representation)\n",
        "    # Add MLP.\n",
        "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
        "    # Classify outputs.\n",
        "    logits = layers.Dense(num_classes)(features)\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=logits)\n",
        "    return model"
      ],
      "metadata": {
        "id": "Z7E9YA4JKBfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(model):\n",
        "    optimizer = tfa.optimizers.AdamW(\n",
        "        learning_rate=learning_rate, weight_decay=weight_decay\n",
        "    )\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "        metrics=[\n",
        "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "    history = model.fit(\n",
        "        generator_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=num_epochs,\n",
        "        validation_data=generator_val,\n",
        "        # callbacks=[callback]\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n"
      ],
      "metadata": {
        "id": "OZSWALSxKH83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_classifier.summary()"
      ],
      "metadata": {
        "id": "ORoEEl9uKP09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "73jgIm8BKVmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "# plt.title('Vision Transformer Training Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc = 'upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-1v9ovOWKbeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "# plt.title('Vision Transformer Training Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc = 'lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "17S59R6dKgbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "predict = vit_classifier.predict(generator_test)\n",
        "predict=np.argmax(predict,axis=1)\n",
        "# y_pred = vit_classifier.predict(generator_test)\n",
        "# y_pred=np.argmax(y_pred,axis=1)"
      ],
      "metadata": {
        "id": "oJWCR0iWKluM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator_test.class_indices.keys()"
      ],
      "metadata": {
        "id": "okM-Y9UcKq9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "clf_report_df = pd.DataFrame(classification_report(generator_test.classes, predict, target_names=['Gallstones', 'Abdomen and retroperitoneum', 'cholecystitis', 'Membranous and gangrenous cholecystitis', 'Perforation', 'Polyps and cholesterol crystals', 'Adenomyomatosis', 'Carcinoma', 'Various causes of gallbladder wall thickening'], output_dict=True, digits=4)).T\n",
        "\n",
        "sns.heatmap(clf_report_df.iloc[:-1, :-1], annot=True, fmt=\".4f\", cmap=sns.cubehelix_palette(as_cmap=True))"
      ],
      "metadata": {
        "id": "jS2onpVgKu9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install sklearn-evaluation"
      ],
      "metadata": {
        "id": "-ZH5SV_jK0Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "array = confusion_matrix(generator_test.classes, predict)\n",
        "df_cm = pd.DataFrame(array, index = ['Gallstones', 'Abdomen and retroperitoneum', 'cholecystitis', 'Membranous and gangrenous cholecystitis', 'Perforation', 'Polyps and cholesterol crystals', 'Adenomyomatosis', 'Carcinoma', 'Various causes of gallbladder wall thickening'],\n",
        "                  columns = ['Gallstones', 'Abdomen and retroperitoneum', 'cholecystitis', 'Membranous and gangrenous cholecystitis', 'Perforation', 'Polyps and cholesterol crystals', 'Adenomyomatosis', 'Carcinoma', 'Various causes of gallbladder wall thickening'])\n",
        "plt.figure(figsize = (6,5))\n",
        "cm_plot = sns.heatmap(df_cm, annot=True, cmap='GnBu', fmt='g')\n",
        "cm_plot.set_xlabel('Predicted Labels')\n",
        "cm_plot.set_ylabel('True Labels')\n",
        "cm_plot.set_title('Confusion Matrix From Self-Attention Based VIT', size=12)"
      ],
      "metadata": {
        "id": "QAsLFjPEK56P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "y_test=generator_test.classes\n",
        "y_pred = predict\n",
        "# y_pred=np.argmax(y_pred,axis=1)\n",
        "\n",
        "target= ['Gallstones', 'Abdomen and retroperitoneum', 'cholecystitis', 'Membranous and gangrenous cholecystitis', 'Perforation', 'Polyps and cholesterol crystals', 'Adenomyomatosis', 'Carcinoma', 'Various causes of gallbladder wall thickening']\n",
        "\n",
        "# set plot figure size\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (12, 10))\n",
        "\n",
        "# function for scoring roc auc score for multi-class\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "\n",
        "    for (idx, c_label) in enumerate(target):\n",
        "        fpr, tpr, thresholds = roc_curve(y_test[:,idx].astype(int), y_pred[:,idx])\n",
        "        c_ax.plot(fpr, tpr, lw=2, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "    c_ax.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "\n",
        "print('ROC AUC score:', multiclass_roc_auc_score(y_test, y_pred))\n",
        "\n",
        "c_ax.legend()\n",
        "c_ax.set_xlabel('False Positive Rate')\n",
        "c_ax.set_ylabel('True Positive Rate')\n",
        "# c_ax.set_title('ROC Curve for nct-crc-he-100k Dataset ', size=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M0HfWFRYLBoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4eB2PYnbLIf1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}